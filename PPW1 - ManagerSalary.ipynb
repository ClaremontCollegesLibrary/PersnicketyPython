{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9275497-a2eb-4fc4-927a-5343df111ea9",
   "metadata": {},
   "source": [
    "# Ask A Manager - Salary Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f5c03-1ff6-4953-a0b2-aa4901b35aa2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Topics & Techniques Covered\n",
    "\n",
    "* CSV vs. Excel files in Pandas\n",
    "* Renaming Columns in a Pandas DataFrame\n",
    "* Converting Strings to Integers\n",
    "* String Methods in Action\n",
    "* Manipulating data with `.apply()` and `lambda`\n",
    "* Filtering rows in Pandas\n",
    "* Using data from multiple DataFrame columns\n",
    "* Data Preprocessing/Integration/Enrichment\n",
    "* `datetime` and `dateutil` (for more, see PPW2 - Doctor Who)\n",
    "* Pandas `.merge()` to join datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb3c3bc-a641-44b8-8972-1442d1119ddd",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc3d99-9410-483e-bcd2-44984aa53ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules from Python Standard Library\n",
    "from datetime import datetime\n",
    "\n",
    "#External modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abce30-82ab-4cf7-b96b-c10d4809bb4f",
   "metadata": {},
   "source": [
    "Our first dataset is a Google Sheet containing survey responses from employees in different management positions at different institutions, from 2021 to present. The survey was conducted as a [blog entry for Ask A Manger](https://www.askamanager.org/2021/04/how-much-money-do-you-make-4.html). The responses are collected in a Google Form and the questions are mostly short-form text response fields, with a few being multiple choice responses. This survey is real-world data, survey participants are all voluntary and self-selected. Most likely, the participants are followers of the Ask A Manager blog and/or the podcast.\n",
    "\n",
    "Not all the responses are recorded in the correct fields, and there is no standardization in the text fields with regard to formatting numbers or abbreviating currencies or regions.\n",
    "\n",
    "This dataset is a *mess*, and as such, it's perfect for practicing several data-cleaning techniques.\n",
    "\n",
    "We are *NOT* going to clean the entire dataset today.\n",
    "\n",
    "Let's say we want to analyze the salary range of the survey's participants, and get summary statistics for the Salary variable. As we'll see, there are several obstacles in our way for accomplishing that goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb53e3b3-eb2b-4c42-9115-cdee978b965f",
   "metadata": {},
   "source": [
    "### Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817beca-65d8-47a0-8899-6b53d7eac130",
   "metadata": {},
   "source": [
    "\n",
    "\"Ask A Manager\" Salary Survey, 2021:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1IPS5dBSGtwYVbjsfbaMCYIWnOuRmJcbequohNxCyGVw/edit?resourcekey=&gid=1625408792#gid=1625408792"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2aefb-41a0-445f-bce0-bdb6e19241fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T21:46:16.117209Z",
     "iopub.status.busy": "2025-02-10T21:46:16.117209Z",
     "iopub.status.idle": "2025-02-10T21:46:16.122664Z",
     "shell.execute_reply": "2025-02-10T21:46:16.121163Z",
     "shell.execute_reply.started": "2025-02-10T21:46:16.117209Z"
    }
   },
   "source": [
    "# Exploratory Data Analysis (EDA) & Data Cleaning\n",
    "\n",
    "You'll often see Exploratory Data Analysis (EDA) and data cleaning described as separate stages of a data science/data analysis project, but most of the time, exploring and cleaning the data are closely intertwined. In order to explore the data and find out what it contains, some cleaning is necessary. And in order to clean data with care, one must know what more about what's in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40468ef-9fc5-4721-aabe-8339070a8143",
   "metadata": {},
   "source": [
    "## Differences between CSV and Excel Imports in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51561b-5107-4c2c-8090-03f727eb25e4",
   "metadata": {},
   "source": [
    "\"Data cleaning\" can begin before you even load the data into Pandas. Often, data is available in different formats, and it is important to know how those formats can behave differently when imported using Pandas. These differences can increase or reduce the amount of effort necessary to make sure data is usable in a statistical model.\n",
    "\n",
    "We have downloaded the \"Ask A Manager\" survey data from the original Google Sheet as both `.csv` (Comma-Separated Values) and `.xlsx` (Microsoft Excel Spreadsheet) files. The data is identical, but it's *represented* differently depending on which format it's imported from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8a66b-e2b5-4c21-bf66-7960150413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read .csv file from The Claremont Colleges Library's GitHub page:\n",
    "df_csv = pd.read_csv('https://raw.githubusercontent.com/ClaremontCollegesLibrary/PersnicketyPython/refs/heads/main/Ask%20A%20Manager%20Salary%20Survey%202021%20(Responses)%20-%20Form%20Responses%201.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c53c4-ce53-4a65-8444-d0a5b4113400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read .xlsx file from The Claremont Colleges Library's GitHub page:\n",
    "df_xl = pd.read_excel('https://raw.githubusercontent.com/ClaremontCollegesLibrary/PersnicketyPython/refs/heads/main/Ask%20A%20Manager%20Salary%20Survey%202021%20(Responses).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc9a504-3363-4908-8a01-cf005ad1005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The \".equals()\" method determines equivalency across two entire DataFrames (or columns of DataFrames). \n",
    "df_csv.equals(df_xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f0139c-351a-485c-8970-d53a2fd38511",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be43f3-1cc7-4fd3-94e9-1216c67cd0ea",
   "metadata": {},
   "source": [
    "### Renaming Columns in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc79ab9-1d86-4a7f-bb66-5f471fcc37e5",
   "metadata": {},
   "source": [
    "Before moving forward, let's declutter those column names to make our comparison a bit more manageable. Part of the data-cleaning process is making quality-of-life adjustments, such as reducing on-screen clutter.\n",
    "\n",
    "We'll take a moment to read the column names and absorb their meaning, and then rename them to shorter column names in a consistent format.\n",
    "\n",
    "We have chosen to label the columns with capitalized words, separated by underscores. Longer words have been abbreviated as a space consideration.\n",
    "\n",
    "Because we are renaming *all* of the columns, we can simply pass in a new list of the same length as the number of columns. This is a quick way to rename the columns given the length of the existing column names.\n",
    "\n",
    "A \"safer\" (i.e.: less error-prone) way of renaming columns would be to use the pandas method `.rename()`. With this method, you can pass a dictionary to the \"columns\" keyword argument (containing old names as keys and new names as values), in this format: `.rename(columns={\"old_name1\":\"new_name1\", \"old_name2\":\"new_name2\"})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53997ab3-0ab0-4f6e-b9b4-9b1ac96a09f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf6f1a-7231-432c-81e8-6f3fe5942e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [\n",
    "    'Timestamp', 'Age', 'Industry', 'Job_Title', 'Addl_Context', 'Salary',\n",
    "    'Addl_Comp', 'Currency', 'Curr_if_Other', 'Income_Context',\n",
    "    'Country', 'US_State', 'City', 'Work_Exp', 'Field_Exp', 'Educ_Level',\n",
    "    'Gender', 'Race'\n",
    "]\n",
    "\n",
    "df_csv.columns = new_columns\n",
    "df_xl.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fea37f-7399-439c-8466-6e9c19fc031c",
   "metadata": {},
   "source": [
    "These new names mostly capture the gist of the original questions. Please keep in mind that Country, US_State, and City all refer to the location of the job, not the location of residence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7959749b-7533-4057-9c06-fd76a7bf6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad587d-f82b-4de3-9660-752905bf3d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_xl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53af65-e11f-4e9a-8be1-5453dfd7f4c3",
   "metadata": {},
   "source": [
    "Right away, we can see that the \"Timestamp\" column looks different. So does \"Salary\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35b927-2e48-4f23-be62-738b4149c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xl['Timestamp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4f2e0-8ac2-49cd-8db3-3d1d51bad322",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Timestamp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e31a24-1fd2-48e1-aafe-88eaf54465e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_xl['Salary'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f8c5a-c6bb-446e-b375-339397631b8d",
   "metadata": {},
   "source": [
    "*(Note: since there is no standardized currency unit, the statistics here for Salary don't really mean anything... yet.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b1f5a-00d8-492f-b099-939483bcd373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Salary'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69358d65-1f99-41f5-863c-db2433de58b1",
   "metadata": {},
   "source": [
    "But the differences aren't limited to those two columns...\n",
    "\n",
    "Unlike the `.equals()` method, using \"==\" on two DataFrames will return a Boolean value for *each cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175e916-9143-4ae3-a889-0f9b46dd077c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_xl == df_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07936f06-cfe0-4110-a305-1164d28a9e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T19:08:52.610035Z",
     "iopub.status.busy": "2025-02-11T19:08:52.610035Z",
     "iopub.status.idle": "2025-02-11T19:08:52.632569Z",
     "shell.execute_reply": "2025-02-11T19:08:52.632569Z",
     "shell.execute_reply.started": "2025-02-11T19:08:52.610035Z"
    }
   },
   "source": [
    "Some of the columns are identical, some of them have no common values, and some of them have some values that match and some that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3e45bc-acb6-4244-8bcd-534ddca65d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865a479-acb6-4133-8177-9d31dac8d69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_xl.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3aff4b-a1bd-420f-ba5a-8ea2770c65ec",
   "metadata": {},
   "source": [
    "Notice that the counts of non-null objects are the same, but Timestamp and Salary show different data types. They are listed as \"objects\" in the .csv import, but `datetime64[ns]` and `int64` respectively in the xlsx import. The Pandas `read_excel()` method automatically reformats dates into a pandas timestamp, whereas `read_csv()` imports them literally as strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bbd60e-534e-42ab-8185-09b11fa583bf",
   "metadata": {},
   "source": [
    "When evaluating Boolean expressions, `NaN` values in Python don't equal *anything*, even other `NaN` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90bc84f-5fbd-428d-aaad-371a5eec40ac",
   "metadata": {},
   "source": [
    "In the original spreadsheet, many (but not all) of the `Salary` figures are formatted using commas to separate thousands.\n",
    "\n",
    "`read_excel()` is built with this contingency in mind, and automatically converts numbers formatted this way into standard integers. `read_csv()` imports the entire column as strings regardless of their original formatting, because at least one of them has a comma in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4922fd22-4e01-4f31-b0eb-cd36730095c9",
   "metadata": {},
   "source": [
    "Because of these differences, importing Excel files will usually be more reliable than importing csv files with respect to data integrity.\n",
    "\n",
    "For this dataset, we *could* just use the imported Excel file rather than the csv file, because it would be easier to work with data that's already formatted the way we want.\n",
    "\n",
    "However, you won't always have access to both csv and .xlsx versions of a spreadsheet in the wild."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978320d8-4580-4ce9-9d8e-274403cf84b6",
   "metadata": {},
   "source": [
    "### *Note: Why not always use Excel files instead of .csv?*\n",
    "\n",
    "Excel files have many advantages over .csv for purposes of data integrity. `.read_excel()` can also handle Excel files with multiple sheets using the [`sheet_name` parameter](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html), which cah be a useful feature.\n",
    "\n",
    "*That said*, Excel files can take significantly longer to import than .csv files. If you are working with a high volume of data, or if your project requires that you repeatedly import files, .csv may be a better format from a computational performance standpoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d851ed-c84c-48fb-a6a8-d75765f95712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T21:26:01.151964Z",
     "iopub.status.busy": "2025-03-07T21:26:01.145965Z",
     "iopub.status.idle": "2025-03-07T21:26:01.215281Z",
     "shell.execute_reply": "2025-03-07T21:26:01.209750Z",
     "shell.execute_reply.started": "2025-03-07T21:26:01.150967Z"
    }
   },
   "source": [
    "# End Goal: Salary & Total Compensation Statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f5afe-5dd5-4ef4-9bb5-97a6b019290f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T21:26:01.151964Z",
     "iopub.status.busy": "2025-03-07T21:26:01.145965Z",
     "iopub.status.idle": "2025-03-07T21:26:01.215281Z",
     "shell.execute_reply": "2025-03-07T21:26:01.209750Z",
     "shell.execute_reply.started": "2025-03-07T21:26:01.150967Z"
    }
   },
   "source": [
    "## Related Columns: \"Currency\", \"Curr_if_Other\", \"Country\", \"Addl_Comp\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b23987-e278-45a2-813f-72d2a144cd74",
   "metadata": {},
   "source": [
    "Our end goal is to get summary statistics for total compensation (Salary + Addl_Comp). To do this, we will actually need data from multiple columns in the dataset, including \"Salary\", \"Currency\", \"Curr_if_Other\", \"Country\", \"Addl_Comp\", and \"Timestamp\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a35550d-00b1-4f62-bbcd-fd639d23ea93",
   "metadata": {},
   "source": [
    "As we saw in `PPW0 - Text Data Cleaning Introduction`, the `.apply()` method and `lambda` statements can be used to edit all values in a column at once\n",
    "\n",
    "Using a lambda function allows us to apply a string method to a DataFrame column, assuming all the data are of the same type.\n",
    "\n",
    "Not all tasks involving data manipulation in Pandas can be accomplished with `.apply(lambda)`, but a huge portion of them can."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1a9e8-f63e-47cd-8e88-5dee7bd669e6",
   "metadata": {},
   "source": [
    "## Salary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc32832e-925b-4e80-90f5-6597a8613ecd",
   "metadata": {},
   "source": [
    "The Salary column is currently in string format. We would like to convert it to integer format so we can get summary statistics from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857afab-5a87-48c3-aa45-7ed1a19bdfef",
   "metadata": {},
   "source": [
    "### EXAMPLE: Cleaning Data using `.apply()` and `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ed26b-f016-4250-9c29-b3a13d775660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Try this!\n",
    "#Uncomment the code below:\n",
    "\n",
    "#df_csv['Salary'] = df_csv['Salary'].apply(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b54b4cf-39a5-4699-a63d-9fb38061b24c",
   "metadata": {},
   "source": [
    "We get `\"ValueError: invalid literal for int() with base 10: '55,000'\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee884847-aa49-4952-ad52-322eeec9b211",
   "metadata": {},
   "source": [
    "### CODING EXERCISE: `.apply()` and `lambda`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764422e0-6d19-43e6-a370-b09dde1f2f20",
   "metadata": {},
   "source": [
    "The `int()` function won't work by itself to convert the Salary field from strings to integers, because of the commas. Unlike pandas `read_excel()`, `int()` isn't set up to automatically handle in strings of numerical characters.\n",
    "\n",
    "What do you think would work best in the lambda statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa56fa11-9572-457e-a4db-fae22a8b285e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try using `.apply()` and `lambda` to clean the Salary column data:\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "df_csv['Salary'] = df_csv['Salary']\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48dde3d-7030-4895-95e1-5ab158f0eb76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### (Solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6aaf13-024d-4bf5-a86e-ec0c7accf821",
   "metadata": {},
   "source": [
    "In this case, all the items in our \"Salary\" column are strings, so we can use the `.replace()` method to clear out the commas, and then wrap the resulting string in an `int()` function to change the type to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc9504-f6c2-41cf-a5ad-d06e4d0f0390",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Salary'] = df_csv['Salary'].apply(lambda x: int(x.replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a727fd6-b352-491f-804c-a68be7f59363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No more commas!\n",
    "df_csv['Salary'].equals(df_xl['Salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ff262-834b-4359-926b-13f508797178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9f01d-c1ec-42fd-90ea-85dcf4cf5c9c",
   "metadata": {},
   "source": [
    "All clean! ...or is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389dccc8-5929-441a-b65a-7c59b78edfd3",
   "metadata": {},
   "source": [
    "### QUESTION: What's missing from the Salary column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8804c0-4292-474a-bf2d-e18f927185c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### (Answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e125a29-0060-493a-ab1d-f72bd671af93",
   "metadata": {},
   "source": [
    "Currency units.\n",
    "\n",
    "In order to normalize the Salary column and produce summary statistics for it, we have to convert the totals to the same currency. We'll go with USD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c5bbd-3c74-42c9-a332-c498b160d727",
   "metadata": {},
   "source": [
    "## Currency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe53647-5d1c-46f9-a0e0-f72ff1a4ec97",
   "metadata": {},
   "source": [
    "`.unique()` is a method for a Pandas Series object that returns all extant values of the series with no duplicates. The order of the series returned matches the order of first appearance starting with the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3d61a8-0b6c-40c3-8a64-3be2a75f0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Currency'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28843869-76ea-496b-83fe-e34ec847a06e",
   "metadata": {},
   "source": [
    "The `.value_counts()` method is similar to `.unique()`, but it returns the count of each value in a Series in descending order. `.value_counts()` is especially useful for determining the composition of categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a6de22-3276-469c-9991-22f4ee5580a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Currency'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486a4ee-1504-4449-814d-5ca3f492dd6a",
   "metadata": {},
   "source": [
    "### What's unusual about the Currency column?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58547f-b96b-4058-8a07-dc71dd567087",
   "metadata": {},
   "source": [
    "* Australian Dollars and New Zealand Dollars aren't the same.\n",
    "\n",
    "* \"Other\" isn't a currency; there's a whole separate column that denotes currency if \"Other\" is what's listed in the Currency column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2ea57a-8bbd-4321-a8b9-db98467e0794",
   "metadata": {},
   "source": [
    "Both of these issues prevent us from simply using the data as it is.\n",
    "\n",
    "Can we still use this data? Let's take a closer look. Let's filter the DataFrame to show only entries with \"AUD/NZD\" listed under Currency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4130519-1ef5-4183-a615-633fa94208eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv[df_csv['Currency'] == 'AUD/NZD']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f29c74-6636-420f-b736-62204d58f9d0",
   "metadata": {},
   "source": [
    "There are 504 rows where the currency is AUD/NZD... that's not a huge number relative to the size of our dataset, but we still don't want to throw out those entries; it would be best if we could salvage that data somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0f357c-fade-4aac-8c42-7f6dbcb63753",
   "metadata": {},
   "source": [
    "### Determining AUD/NZD by Country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb554a-e569-451e-a978-602ad78984ca",
   "metadata": {},
   "source": [
    "We can try to solve this issue by splitting the AUD/NZD category based on the user's answer in the `Country` field. This may not work for all of the entries in question, but it will improve the accuracy of our summary statistics.\n",
    "\n",
    "To get a list of countries where the currency is listed as AUD/NZD, we can use `.tolist()`, which converts a Pandas Series object (such as what's returned by the `.unique()` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f9b8d-3efb-4144-80a9-9180a394d976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv[df_csv['Currency'] == 'AUD/NZD']['Country'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352539b-cae3-4e37-8faa-650847acdcd0",
   "metadata": {},
   "source": [
    "We got mostly what we expected: variations on Australia and New Zealand with different spelling, capitalization, extra whitespace, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b05f8c-dcad-4646-9499-224b87aa54ff",
   "metadata": {},
   "source": [
    "However, there are three results that warrant further investigation: USA, Canada, and \"From New Zealand but on projects across APAC\". In a dataset this large, three entries is an incredibly small number, and likely won't have a huge impact on our summary statistics, but for purposes of demonstration, we're going to try to correct them here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc0bbd-681e-412d-b47e-f0f98e1416c6",
   "metadata": {},
   "source": [
    "### Filtering DataFrames on Multiple Conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55674d82-242d-416b-9125-a3f87a5a23e4",
   "metadata": {},
   "source": [
    "Multiple conditions can be applied to filter DataFrames. In order to filter on multiple conditions, each condition (a Series object of the same length as the DataFrame) should be wrapped with parentheses. \n",
    "\n",
    "Use \"&\" and \"|\" instead of \"and\" and \"or\" in order to combine multiple conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf9ebb-1ad8-4684-8d56-9046efdc57fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[(df_csv['Currency'] == 'AUD/NZD') & (df_csv['Country'] == 'USA')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ec883-15c3-4149-8278-f4fd9e1de278",
   "metadata": {},
   "source": [
    "Since filter conditions are Series objects, they can be stored as variables. When we do this, we don't have to use parentheses for multiple conditions.\n",
    "\n",
    "We won't always use this syntax, but it can help make filtering a much tidier process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8464e42f-79a5-4bb4-9ce6-0bee9d651b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Pandas Series objects with True/False values for each condition\n",
    "currency_audnzd = df_csv['Currency'] == 'AUD/NZD'\n",
    "country_canada_or_apac = (df_csv['Country'] == 'Canada') | (df_csv['Country'] == 'From New Zealand but on projects across APAC')\n",
    "\n",
    "#Apply conditions\n",
    "df_csv[currency_audnzd & country_canada_or_apac]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63528b98-f30b-4d1f-9838-86686249d2c4",
   "metadata": {},
   "source": [
    "Neither the American nor the Canadian respondent indicates which country's currency they are paid in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd117bd-8fe9-4c88-847e-eff8f09bd34d",
   "metadata": {},
   "source": [
    "We're going to assume this last one is \"NZD\" as a default since the respondent lives in New Zealand, but the Salary figure may or may not represent 100% New Zealand Dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4cdab6-6f93-40af-bedd-c1c0e65b969d",
   "metadata": {},
   "source": [
    "### *Hold That Thought*\n",
    "\n",
    "Before we continue with cleaning our AUD/NZD currencies, we're going to take a look at the \"Curr_if_Other\" column and determine if more cleaning is necessary before we integrate it into the \"Currency\" column. \n",
    "\n",
    "Because we're working with multiple columns of data, we want to make sure that we're applying the same standards for data cleaning across those columns. As we'll see in a moment, the \"Curr_if_Other\" column becomes much easier to work with if all the values are set to lower case, and leading and trailing whitespace is removed. Since we're doing this with \"Curr_if_Other\", it also makes sense to follow the same procedure with other columns, *before* we begin to substitute currency values based on the country of residence.\n",
    "\n",
    "Once we've done that, we can return to resolving the AUD/NZD issue.\n",
    "\n",
    "Cleaning data is seldom a purely linear process, and it's quite common that one process will need to be put on hold until another is completed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b0cf7-94e7-4f8b-b317-078f4d0b5c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-04T22:46:56.535043Z",
     "iopub.status.busy": "2025-03-04T22:46:56.535043Z",
     "iopub.status.idle": "2025-03-04T22:46:56.543921Z",
     "shell.execute_reply": "2025-03-04T22:46:56.541915Z",
     "shell.execute_reply.started": "2025-03-04T22:46:56.535043Z"
    }
   },
   "source": [
    "## Curr_if_Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a78106-cf65-4b73-a044-11a4f52f8740",
   "metadata": {},
   "source": [
    "Counting AUD and NZD separately, there are only 11 currencies listed in the Currency column... but then there's \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469c8f28-ecf4-4f30-b41a-80479e8dba05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv[df_csv['Currency'] == 'Other']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4fd049-b984-46a6-9526-c1a839294fc3",
   "metadata": {},
   "source": [
    "Using the same `.value_counts()` method as before, we can apply the `.head()`method to get a specific number of categories. If no argument is passed to `.head()`, it will display 5 values, but we want to see more currencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478653e4-42ca-4bd8-9afc-2e19282796b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Curr_if_Other'].value_counts().head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9bbca8-57c1-4ec3-a0c9-2b58cb462a7d",
   "metadata": {},
   "source": [
    "That's a lot of different values, some of which are clearly extraneous. \n",
    "\n",
    "Because Python is case-sensitive, \"ILS\" and \"Ils\" (Israeli New Shekels) are treated as different categories.\n",
    "\n",
    "We can make it easier to match up categories in the Curr_if_Other column (and several others) by lowering the case of all the strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224291b5-5e73-4ce4-8f03-ce9ba49bda79",
   "metadata": {},
   "source": [
    "## Removing Whitespace and Converting to Lower Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcbec1a-f8d5-4c75-8da4-35bc53a615a5",
   "metadata": {},
   "source": [
    "Normalizing our survey responses will be easier if we can remove excess whitespace and convert all text to lower case:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a8b3a-4680-4eb7-846d-5e1ad539f03b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T18:35:00.019498Z",
     "iopub.status.busy": "2025-03-31T18:35:00.019498Z",
     "iopub.status.idle": "2025-03-31T18:35:00.033910Z",
     "shell.execute_reply": "2025-03-31T18:35:00.030860Z",
     "shell.execute_reply.started": "2025-03-31T18:35:00.019498Z"
    }
   },
   "source": [
    "### CODING EXERCISE - Lowercase & Removing Whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058a388-1bd1-46c6-964b-b7578b786982",
   "metadata": {},
   "source": [
    "Recall the string methods from PPW0 - Text Data Cleaning Introduction and use them in a .apply(lambda) statement to clean the Country column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933edd0-3204-4c6a-980b-37920862681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "df_csv['Country'] = df_csv['Country']\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03ba797-2fa7-4b4f-abcd-06b4d8f7a91b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### (Solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbbf1f6-d4ce-4c78-bf08-8707173d41f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Country'] = df_csv['Country'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b97574-79b7-4c1a-add2-6419aaba003c",
   "metadata": {},
   "source": [
    "This will make it simpler to do substitutions on the AUD/NZD currencies. Remember the list of countries we created earlier? If we run the same code again, the list is shorter, since differences in capitalization and extra spaces have been cleaned up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a2c80a-f567-48db-b4db-d8b723d7d28d",
   "metadata": {},
   "source": [
    "### Reduced List of Country Labels for AUD/NZD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96178c-14be-4345-bc38-892689bfa864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Currency'] == 'AUD/NZD']['Country'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d1f60-9a5a-4c2c-b2da-4c428f9bc4d0",
   "metadata": {},
   "source": [
    "Now there are only eight elements in the list instead of fourteen. This step has reduced the number of unique values we will have to account for when we actually perform the substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f78e6e-c1b1-4dc0-885f-47c731b00d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Currency'] = df_csv['Currency'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df36a1e8-8901-463a-a1af-9213865675ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Currency'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec965dc0-7b87-4bb3-b548-98116bb20934",
   "metadata": {},
   "source": [
    "What happens when we try to use this on the Curr_if_Other column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe8fb07-f5fe-400b-937b-bbb1d1cf7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Try it!\n",
    "# Uncomment the line of code below and try running it to see what happens:\n",
    " \n",
    "#df_csv['Curr_if_Other'] = df_csv['Curr_if_Other'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fa383a7-be98-4279-9d70-3b9b0856f4ee",
   "metadata": {},
   "source": [
    "Another error! What does that mean? We don't have any floating-point numbers in that column... do we?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7a9ec-1d98-4349-9d47-8477973bb835",
   "metadata": {},
   "source": [
    "### `NaN` is \"Not a Number\"... But it is a Float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17a761-bac6-4486-a3b6-d53df18710f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T20:07:47.214002Z",
     "iopub.status.busy": "2025-02-12T20:07:47.212994Z",
     "iopub.status.idle": "2025-02-12T20:07:47.249060Z",
     "shell.execute_reply": "2025-02-12T20:07:47.245518Z",
     "shell.execute_reply.started": "2025-02-12T20:07:47.214002Z"
    }
   },
   "source": [
    "It turns out that null values (\"NaN\" or \"Not a Number\") are treated as floating point numbers in Python. \n",
    "\n",
    "Empty cells in the .csv file are not treated as `''`, but rather as null values. This means we can't use string methods on them, and we'll have to find a means of converting them to empty strings instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec702a-1e89-4f06-9d19-be6e4b9fc33a",
   "metadata": {},
   "source": [
    "Luckily, the `.fillna()` method is just that! This lets us pass a dictionary with the name of the column and the value we want to summarily replace the `NaN`s with.\n",
    "\n",
    "*Note: If you've been using Pandas for a while, this may look different from the way you learned; please see the `PPW5 - Odds and Ends` notebook. If you're just starting out in Pandas, feel free to move on.*\n",
    "\n",
    "The `inplace` parameter tells the method whether or not the changes should occur in the original object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ff87e-8239-4755-8b58-0793450cd0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.fillna({'Curr_if_Other':''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8be25-514f-4b5d-bd83-dd593d5c72dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Curr_if_Other'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fe6ce-f9c7-403c-b7b5-02f9b54400cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-12T20:11:55.668448Z",
     "iopub.status.busy": "2025-02-12T20:11:55.667448Z",
     "iopub.status.idle": "2025-02-12T20:11:55.678121Z",
     "shell.execute_reply": "2025-02-12T20:11:55.677111Z",
     "shell.execute_reply.started": "2025-02-12T20:11:55.668448Z"
    }
   },
   "source": [
    "Now there are no null values in the Curr_if_Other column. Let's try that same `.apply()` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66c71c-88da-4d06-949d-f7117aa924e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Curr_if_Other'] = df_csv['Curr_if_Other'].apply(lambda x: x.lower().strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9618-7f24-4faf-86f9-078a48ca251c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-26T19:38:16.130832Z",
     "iopub.status.busy": "2025-03-26T19:38:16.129834Z",
     "iopub.status.idle": "2025-03-26T19:38:16.136356Z",
     "shell.execute_reply": "2025-03-26T19:38:16.134833Z",
     "shell.execute_reply.started": "2025-03-26T19:38:16.130832Z"
    }
   },
   "source": [
    "### *Picking Up Where We Left Off...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1e9d3-842a-44c4-9f13-01b685fb1786",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "au_nz_list = df_csv[df_csv['Currency'] == 'aud/nzd']['Country'].unique().tolist()\n",
    "au_nz_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01edf19-372f-4214-a6e7-afb593b4b379",
   "metadata": {},
   "source": [
    "We now have a list of country names for respondents who are paid in \"AUD/NZD\". We can use this list to create a dictionary to further standardize the values in this field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279bfbd-b74a-4560-a03f-85e52c2880c8",
   "metadata": {},
   "source": [
    "#### Programming the lazy way:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e54df2-347f-4179-afbc-30425504e368",
   "metadata": {},
   "source": [
    "A neat little trick we can do in Python to avoid having to retype code is to write a chunk of code that prints out components of code that we can use later on.\n",
    "\n",
    "For this, we can plug the list of \"Country\" values associated with AUD/NZD into a dictionary comprehension (yes, those exist!), and then use the printout as a template to insert the names we actually want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce535eb6-e074-46d3-ba76-e772d0d76b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary with empty strings as values:\n",
    "au_nz_dict = {country: '' for country in au_nz_list}\n",
    "au_nz_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f0350-708f-4b81-aca8-78c76272c9f6",
   "metadata": {},
   "source": [
    "### CODING EXERCISE - Replacement Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e542411-6d58-4b14-8b94-c7346f65cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy-paste the contents of \"au_nz_dict\" here into the cell, replacing the empty \"{}\"\n",
    "#then insert the values you think are appropriate to each country:\n",
    "# Keep in mind that we'll be applying the dictionary to the whole column,\n",
    "# not just entries that have aud/nzd for currency:\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "au_nz_dict = {}\n",
    "\n",
    "au_nz_dict\n",
    "\n",
    "################################################################################\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff99b6-7d7a-4c4e-b9f7-02fe2b682d81",
   "metadata": {},
   "source": [
    "#### (Solution) - Completed Dictionary (Click to Reveal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ef649b-8a33-4345-9124-1689cd8efe2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Copy-paste that here, then insert the values appropriate to each country:\n",
    "\n",
    "au_nz_dict = {'australia': 'australia',\n",
    " 'new zealand': 'new zealand',\n",
    " 'new zealand aotearoa': 'new zealand',\n",
    " 'nz': 'new zealand',\n",
    " 'australi': 'australia',\n",
    " 'canada': 'canada',\n",
    " 'from new zealand but on projects across apac': 'new zealand',\n",
    " 'usa': 'usa',\n",
    " 'aotearoa new zealand': 'new zealand'}\n",
    "\n",
    "au_nz_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c2441-f0cc-4f12-826b-28e4bb8ba4cc",
   "metadata": {},
   "source": [
    "## Conditional Statements Inside Lambdas:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36262d72-aae6-4031-beb3-5f8bfb8c7fc7",
   "metadata": {},
   "source": [
    "Lambda statements can also make use of \"if/else\" statements. The way they're ordered is different from the standard implementation of an if/else block in a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c17c836-8d80-49b3-8711-44436f8fa2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a `for` loop:\n",
    "\n",
    "#for x in df_csv['Country']:\n",
    "#    if x in au_nz_dict.keys():\n",
    "#        print(au_nz_dict[x])\n",
    "#    else:\n",
    "#        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0dba2f-f9b4-4202-ac2a-84faf7c909a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a `lambda` statement:\n",
    "df_csv['Country'] = df_csv['Country'].apply(lambda x: au_nz_dict[x] if x in au_nz_dict.keys() else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af3224-e7ac-470d-a13a-cb6f89c631d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T23:47:34.351850Z",
     "iopub.status.busy": "2025-03-07T23:47:34.351850Z",
     "iopub.status.idle": "2025-03-07T23:47:34.360858Z",
     "shell.execute_reply": "2025-03-07T23:47:34.360313Z",
     "shell.execute_reply.started": "2025-03-07T23:47:34.351850Z"
    }
   },
   "source": [
    "Let's check how many countries show up now for aud/nzd entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41b3ee4-c887-48f1-a6f8-9cdcbac44899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv[df_csv['Currency'] == 'aud/nzd']['Country'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fc6037-54fb-4083-89b3-11983be6750d",
   "metadata": {},
   "source": [
    "### `.apply(lambda)` Across Multiple Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5799bc55-3e21-47f6-9e07-c1093ddfa860",
   "metadata": {},
   "source": [
    "From here, how do we convert the Currency to AUD and NZD from AUD/NZD based on country?\n",
    "\n",
    "We can use another `.apply(lambda)` statement, but this time, since we need to use data from both the Currency and Country columns, we will have to pass a custom function to `.apply(lambda)` that can accept multiple inputs.\n",
    "\n",
    "For the two entries for which the Currency is listed as AUD/NZD but the Country is neither Australia nor New Zealand, we don't have enough additional context to determine the currency, so we're going to change their label to \"other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca57982-2d65-4169-9953-5ddebc6ef395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_aud_nzd(currency, country):\n",
    "    \"\"\"\n",
    "    Check if currency is 'aud/nzd'\n",
    "        If so, check country\n",
    "            Match currency to country if applicable\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if currency == 'aud/nzd':\n",
    "        \n",
    "        if country == 'australia':\n",
    "            currency = 'aud'\n",
    "            \n",
    "        elif country == 'new zealand':\n",
    "            currency = 'nzd'\n",
    "            \n",
    "        else:\n",
    "            currency = 'other'\n",
    "    \n",
    "    return currency\n",
    "\n",
    "# \"axis=1\" makes `.apply()` apply the function across a row rather than down a column \n",
    "df_csv['Currency'] = df_csv.apply(lambda x: currency_aud_nzd(x['Currency'], x['Country']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaead39-3bef-462c-9dee-759dbf06d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Currency'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a0effd-c760-444e-bb6b-5dd1eeb6a787",
   "metadata": {},
   "source": [
    "We will return to "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d915f64-19e1-4d19-9289-786862283f5e",
   "metadata": {},
   "source": [
    "## Addl_Comp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f29c5dd-1dad-4be4-b3f3-a1f747df4e14",
   "metadata": {},
   "source": [
    "Last among our Ask A Manager data columns, we need to fill `NaN` values in Addl_Comp with zeroes. This will eventually let us compare summary statistics for base salary with those for total compensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71534fdb-bd8b-41a5-91b3-c9a295030981",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Addl_Comp'] = df_csv['Addl_Comp'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0c0336-5977-4d13-a147-ad8a40b54353",
   "metadata": {},
   "source": [
    "# Replacing \"other\" with Value from Curr_if_Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813bcfe6-6f7b-4671-ba9e-ddb84461c446",
   "metadata": {},
   "source": [
    "## Dictionary for Currency Replacement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6085dfcb-26f7-42af-9cc1-46d5b3675127",
   "metadata": {},
   "source": [
    "Below is the *beginning* of a dictionary that one could use to replace values in the original dataset. We're not going to replace every single value today, but this should give you an idea of the kind of fine-tuning one *can* do with a dataset like this if necessary.\n",
    "\n",
    "It is not always useful or advisable to clean a dataset entirely, as there is a trade-off between getting a task done perfectly and getting a task done at all. However, many practicing data scientists report that the majority of their time is spent cleaning data, as described in greater detail in this [Forbes article from 2016](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2764fb7f-5eef-44ba-9ea9-166d78001324",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_dict = {\n",
    "    'canadian':'cad',\n",
    "    'ntd':'twd',\n",
    "    'danish kroner':'dkk',\n",
    "    'converted mine into usd for your easyness':'usd',\n",
    "    'php (philippine peso)':'php',\n",
    "    'philippine pesos':'php',\n",
    "    'argentine peso':'ars',\n",
    "    'argentinian peso (ars)':'ars',\n",
    "    'inr (indian rupee)':'inr',\n",
    "    'israeli shekels':'ils',\n",
    "    'it’s marketed as £22000 but we get paid pro-rats, so no pay for the school holidays.':'gbp'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f595ecc-1b4c-4acd-acd6-5f2efbd4297a",
   "metadata": {},
   "source": [
    "Once again, we're using a conditional statement inside a lambda statement in order to replace specific values. Rather than editing the \"Curr_if_Other\" column in-place, we're preserving the original column and creating a new column that will contain the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcddbc-5833-45b4-803f-5e13d128967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv['Curr_if_Other_clean'] = df_csv['Curr_if_Other'].apply(lambda x: currency_dict[x] if x in currency_dict.keys() else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97934ca7-f904-4226-b74f-b21638fd82f0",
   "metadata": {},
   "source": [
    "We need to use multiple columns of our DataFrame in order to get all the currencies in one column. \n",
    "\n",
    "For this, we must define a new function that will let us replace \"other\" in the Currency column with whatever value is in the Curr_if_Other column, as we did before with AUD/NZD using both the Currency and Country columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4db45-7468-4a4a-9501-e99c8ebbdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def currency_combine(currency, other_currency):\n",
    "    \"\"\"\n",
    "    \n",
    "    replaces currency == 'other' with currency abbreviation from other_currency\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    value = currency\n",
    "    if value == 'other':\n",
    "        value = other_currency\n",
    "\n",
    "    return value\n",
    "\n",
    "df_csv['Currency_clean'] = df_csv.apply(lambda x: currency_combine(x['Currency'], x['Curr_if_Other_clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a71164-6e3e-4e37-89d4-26e0b47b40ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv['Currency_clean'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aedbe2d-0f40-43a0-a738-78d9dfc15d65",
   "metadata": {},
   "source": [
    "Next, we'll need to incorporate data from another dataset - one that gives us historical exchange rate data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90cd95-c477-49bf-8ead-c094381b5c0b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3d9b4-2f35-46d3-b630-1600af151d27",
   "metadata": {},
   "source": [
    "In data science problems, columns in a dataset are sometimes referred to as \"[features](https://en.wikipedia.org/wiki/Feature_(machine_learning))\". Features are the discrete characteristics of a dataset. \"Feature\" is often used somewhat interchangeably with \"explanatory variable\"/\"independent variable\" to describe individual categories of data that serve as the input for a machine learning and/or statistical model.\n",
    "\n",
    "Often, the existing features of a dataset are adequate but not optimal for creating a model. In such cases, it is sometimes possible to use relationships between different variables in one's dataset to create new features or update existing ones so they are more appropriate inputs. This is often called data transformation or \"[feature engineering](https://en.wikipedia.org/wiki/Feature_engineering)\". One may also pull in data from another source and combine it with the existing data. This process is often referred to as data integration or data enrichment (although those terms can have slightly different meanings depending on who you're talking to, what field they work in, and the nature of the data). \n",
    "\n",
    "All of these fall under the broader category of [data preprocessing](https://en.wikipedia.org/wiki/Data_preprocessing), which encompasses all the steps from loading the data, cleaning it, transforming it, and combining it with data from other sources, before the modeling stage.\n",
    "\n",
    "In our dataset, we have Salary as a numerical feature and Currency as a categorical feature, but we can't really use either of them to get our summary statistics unless we combine them.\n",
    "\n",
    "We also have a column for additional compensation, which reflects compensation not included in base salary. Not every respondent has a figure for this field, but we'd like to be able to use it where applicable.\n",
    "\n",
    "Towards these goals, we need to find a means of converting currencies so we can interpret the salary all in one currency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371f9f2-494e-480a-a28d-1c198d390e6a",
   "metadata": {},
   "source": [
    "## Compustat Global Historical Currency Exchange Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc2d05-ce8a-4848-9595-600bee404ae9",
   "metadata": {},
   "source": [
    "When converting foreign currencies to US Dollars, it can be helpful to keep in mind that the exchange rates are not static. Fortunately for us, there are organizations that pay *very close attention* to fluctuations in exchange rates.\n",
    "\n",
    "The Claremont Colleges Library has a subscription to Compustat Global through Wharton Research Data Services (WRDS). Compustat offers historical currency conversions. A query to this database will allow us to get currency conversion rates for the time period in the dataset (2021-2025), and apply currency exchange rates appropriate to the timestamps recorded when participants of the survey responded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b430b38e-1290-4e12-b867-cc2665e39691",
   "metadata": {},
   "source": [
    "After exporting the data from Compustat, we have an Excel file that contains currency abbreviations, date, and rate to US Dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffbc006-ce76-4d25-9aac-bd6446446207",
   "metadata": {},
   "source": [
    "### Secondary Data Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49473851-bd09-4605-b8e2-9d574cb54025",
   "metadata": {},
   "source": [
    "We've exported a small subset of Compustat Global's historical exchange rates to help us enrich our existing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ff384-3823-4cbd-86b7-371298db0346",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch = pd.read_excel('https://raw.githubusercontent.com/ClaremontCollegesLibrary/PersnicketyPython/refs/heads/main/CompustatExchangeRates.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda507b3-4c5f-4ea4-8f2f-94ecf7eea102",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f197b-7f03-429b-8a6c-0d5ffb11abdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T19:23:20.791001Z",
     "iopub.status.busy": "2025-03-31T19:23:20.791001Z",
     "iopub.status.idle": "2025-03-31T19:23:20.822659Z",
     "shell.execute_reply": "2025-03-31T19:23:20.820644Z",
     "shell.execute_reply.started": "2025-03-31T19:23:20.791001Z"
    },
    "scrolled": true
   },
   "source": [
    "The exchange rates for AUD to USD and for NZD to USD have changed significantly over four years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5786e-53cc-4a7c-b6f7-19b6a9996c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exch[exch['Currency'] == 'AUD']['Data Date'], exch[exch['Currency'] == 'AUD']['Rate to USD'])\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('AUD to USD')\n",
    "plt.title('AUD - Historical Exchange Rate to USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f3b46-564b-4413-9418-1247a9487d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(exch[exch['Currency'] == 'NZD']['Data Date'], exch[exch['Currency'] == 'NZD']['Rate to USD'])\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('NZD to USD')\n",
    "plt.title('NZD - Historical Exchange Rate to USD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9664d66d-a3b8-4b2b-8fc8-bf9006d89bcf",
   "metadata": {},
   "source": [
    "To make merging the DataFrames easier, we can convert all the currency abbreviations to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c6819-1b2e-4fab-8ebf-78b61a7532f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch['Currency'] = exch['Currency'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cfe1e6-3667-4530-9d09-6793311fc27f",
   "metadata": {},
   "source": [
    "## List of Currency Abbreviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292fdff9-a51e-4363-8662-1f213b7e5e0a",
   "metadata": {},
   "source": [
    "Reference for abbreviations: https://www.foreignexchangelive.com/currency-codes-symbols/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac3cbf-57d1-4d27-bed6-90f6ba709a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "currencies = exch['Currency'].unique().tolist()\n",
    "currencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b7312a-d2b7-4955-bd4f-df4dc825aed5",
   "metadata": {},
   "source": [
    "## Currencies not in Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300044aa-d9ae-4369-83a6-00175dba5d6a",
   "metadata": {},
   "source": [
    "Given that there is no dropdown menu to select currency in the \"Currency if Other\" field of the survey, there will be some entries that do not conform to the standard three-character currency abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538515e5-6a3e-455c-93ec-a7168f268a99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize empty list\n",
    "currency_outliers = []\n",
    "\n",
    "#Iterate through \"Curr_if_Other\" column\n",
    "for currency in df_csv['Curr_if_Other_clean'].tolist():\n",
    "\n",
    "    #Check against list of currencies from Compustat\n",
    "    if currency not in currencies:\n",
    "\n",
    "        #Add currency to outliers list if it's not blank or already there\n",
    "        if currency != '':\n",
    "            \n",
    "            currency_outliers.append(currency)\n",
    "            \n",
    "#Print length of outlier list            \n",
    "print(\"Nonstandard currency entries: \",len(currency_outliers))\n",
    "\n",
    "currency_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c67c63-ae9c-4ad5-b647-1ccf4c01f24e",
   "metadata": {},
   "source": [
    "64 isn't a terribly high number of entries requiring correction, considering that there are 28106 entries in total.\n",
    "\n",
    "We've already used a dictionary to substitute some of these values in the \"Curr_if_Other_clean\" column. If you'd like to be extra-thorough, you can go back to the dictionary `currency_dict` and replace more values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df6b1ef-6f50-474b-941e-f0296ad48889",
   "metadata": {},
   "source": [
    "If we look at specific values, we can also find some records with erroneous data entries. Some of them look like user error, but others look pretty deliberate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88775942-d636-4e44-8613-375cec89990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c012ee-627e-4b6d-9d3a-8f2fc6029011",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='ekignkfb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210c89fb-4a0a-4ecc-b3e7-c7f1cb60d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='rice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150f89e2-e493-4d7e-af23-f42c738c941a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='55,000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393882ab-a623-46f1-bd77-0b5c149b1f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='6000 in stock grants annually']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1e6d3-ae3c-4740-9a34-3c9d993e946f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_csv[df_csv['Curr_if_Other']=='47000']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fc235-be6e-4151-9322-2813d8dc5976",
   "metadata": {},
   "source": [
    "# Trimming the `df_csv` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a78e54-830a-4324-8c74-b0d4539b4b3f",
   "metadata": {},
   "source": [
    "We're going to get rid of all the entries that don't have an official currency designation. The new DataFrame will be called `df_subset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495c44f-5e1a-47ec-9e8d-054754839286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df_csv[df_csv['Currency'].isin(currencies)].reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0da6e-076f-4c77-8ee3-6d387b7ced7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6845f-80cb-4496-8ce7-aa02f4bf378b",
   "metadata": {},
   "source": [
    "# Merging the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816a4da-6b91-4c6f-b115-f22dd4e13569",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch['Data Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21a73e-1a65-4904-aa63-e0ff009f4480",
   "metadata": {},
   "source": [
    "The Compustat dataset has dates formatted \"YYYY-MM-DD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6bbbb4-75c6-4848-a398-e58ab748c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['Timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab220f8-7b5a-4d6b-982f-5aaadfa23f4f",
   "metadata": {},
   "source": [
    "The timestamps in df_csv/df_subset are formatted \"MM/DD/YYYY HH\\:MM:SS\". We will need to change these to the same format in order to merge our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afd4b5-0796-412b-832d-053ef2bd2cf1",
   "metadata": {},
   "source": [
    "## Timestamps: Python `datetime` and `dateutil`\n",
    "\n",
    "Python's native `datetime` module can handle a large number of date formats, but it's syntax isn't all that intuitive. The [`dateutil`](https://dateutil.readthedocs.io/en/stable/) module adds additional functionality to `datetime`, including a parser that can read dates from a variety of formats.\n",
    "\n",
    "For more on these modules, see the first bonus notebook of this workshop, `PPW2 - Doctor Who.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced03ad0-1cf7-454d-94f3-8254a5642172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Format the original Timestamp string as a datetime object:\n",
    "df_subset['Timestamp_YMD'] = df_subset['Timestamp'].apply(lambda x: dateutil.parser.parse(x))\n",
    "\n",
    "#Reformat the resulting datetime object as a string that matches the currency exchange data format:\n",
    "df_subset['Timestamp_YMD'] = df_subset['Timestamp_YMD'].apply(lambda x: datetime.strftime(x, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5dce96-d1fa-407a-b9e7-85a338c2293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['Timestamp_YMD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284971f-e91d-4ab4-b836-e70d697461a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1dbbf8-695d-4e77-8513-dc3486efe6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ad651-5820-4c42-b53b-868fe4cae460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are renaming the columns in the Compustat exchange rate dateset so we can merge onto the same columns in our survey:\n",
    "exch.rename(columns={'Data Date':'Timestamp_YMD', \"Currency\":'Currency_clean'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6557fa0d-cf17-4c0e-bfb2-87c0568e7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.strftime formats a timestamp into a specified string format, in this case, YYYY-MM-DD\n",
    "exch['Timestamp_YMD'] = exch['Timestamp_YMD'].apply(lambda x: datetime.strftime(x, '%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8909cc-ed21-4fce-919b-008e7dfad1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "exch['Timestamp_YMD'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc84c7c4-a951-47bd-bc40-5fa3869038cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset['Timestamp_YMD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26d06ec-5175-44d3-bc96-bfdca94911d4",
   "metadata": {},
   "source": [
    "## Pandas `.merge()`\n",
    "\n",
    "[Pandas `.merge()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.merge.html) is a method that allows us to merge two dataframes on columns they have in common, in one of several distinct ways. This is essentially the same as a JOIN function with SQL, just written in a different syntax.\n",
    "\n",
    "`.merge()` is one of the more powerful methods in Pandas, as it allows data from multiple sets to be combined to create a richer dataset.\n",
    "\n",
    "In this case, we will be performing what's called a \"left join\", where we are adding values from the second DataFrame to the first DataFrame *only* where there are existing matching values. We are merging on two columns, currency and timestamp, so that for every survey response, we will get the exchange rate for each specific currency to USD on each specific day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab4b89b-c5aa-490c-8d1f-3195fe3d6a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(left=df_subset, right=exch, how='left', on=['Currency_clean', 'Timestamp_YMD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee63f7e-0172-447a-819e-04420c66b75a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47c6d6-e018-41c5-9f25-6a5dfaf72d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Rate to USD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5def8720-db6b-43b8-84f2-8c9a86f563ef",
   "metadata": {},
   "source": [
    "Our summary statistics don't tell us much about \"Rate to USD\", since most of the entries are already USD. However, we'll be multiplying the Salary and Total_Comp variables by these exchange rates in order to get our normalized compensation variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3013d2fc-8a05-4e79-9494-6352f8b51c5a",
   "metadata": {},
   "source": [
    "# The Payoff - Salary Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91960fd-e139-4fa7-b1f2-eb50878f3473",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Total_Comp'] = merged['Salary'] + merged['Addl_Comp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2de80-4233-4297-9582-1d9e0ba86ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Salary_USD'] = merged['Salary'] * merged['Rate to USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2dd66-b236-4659-aa5a-8479bb2f6558",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['Total_Comp_USD'] = merged['Total_Comp'] * merged['Rate to USD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846408c-3ea8-4ee9-b760-5bf930ab8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['Salary','Total_Comp','Salary_USD', 'Total_Comp_USD']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afe3676-62ee-4266-8415-481778803a62",
   "metadata": {},
   "source": [
    "Before converting to a unified currency, summary statistics for \"Salary\" and \"Total_Comp\" were meaningless. Adj_Salary_USD and Adj_Total_Comp_USD have been normalized to reflect the exchange rate from the original currency to USD at the time the survey was taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7aec15-3894-4ac2-b0ab-2ef76de76375",
   "metadata": {},
   "source": [
    "# Wrap-up & Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707c73a4-9075-4d62-866c-0b18918ac3af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## What else do you think we could do to improve the quality of the data and/or the accuracy of these statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c872ee2-8961-4dfc-b08c-805d90871dd3",
   "metadata": {},
   "source": [
    "## What other sorts of data analysis do you think we could do with the data in this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c271c-7d42-4841-a70a-7ef1d868ebd5",
   "metadata": {},
   "source": [
    "# Other Example Projects Using the \"Ask A Manager\" Dataset\n",
    "\n",
    "A quick search yielded a few projects other people had done to analyze this dataset.\n",
    "\n",
    "What are their goals? What do they do differently? How do they address the problem of cleaning the data?\n",
    "\n",
    "https://annasanders.github.io/ms_projects/dtsa_5505/Data_Mining_Project_Report_final.pdf\n",
    "\n",
    "https://github.com/brightboy373/Cleaning-and-Exploring-the-Ask-a-Manager-Survey-Dataset\n",
    "\n",
    "https://github.com/maggiewolff/ask-a-manager-salary-survey\n",
    "\n",
    "https://github.com/shaecodes/Ask-A-Manager\n",
    "\n",
    "This one is done in R:\n",
    "\n",
    "https://jtr13.github.io/cc19/ask-a-manager-salary-survey-dataset.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfd8782-1df1-42ff-9397-d394dd204efa",
   "metadata": {},
   "source": [
    "# End of Module 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465ad72-67bc-4c8e-aa2d-bf9f00d81c07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
